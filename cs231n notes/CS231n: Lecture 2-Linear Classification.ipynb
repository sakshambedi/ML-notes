{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "\n",
    "## Parameterized Mapping from images to label score\n",
    "The first component of this approach is to define the score function that maps the pixel values of an image to confidence scores for each class. We will develop the approach with a concrete example. As before, let's assume a training dataset of images $x_i \\in \\mathbb{R}^D$, each associated with a label $y_i$. Here $i = 1, \\ldots, N$ and $y_i \\in \\{ 1, \\ldots, K \\}$. That is, we have $N$ examples (each with a dimensionality $D$) and $K$ distinct categories. For example, in CIFAR-10 we have a training set of $N = 50,000$ images, each with $D = 32 \\times 32 \\times 3 = 3072$ pixels, and $K = 10$, since there are 10 distinct classes (dog, cat, car, etc). We will now define the score function $f : \\mathbb{R}^D \\to \\mathbb{R}^K$ that maps the raw image pixels to class scores.\n",
    "\n",
    "### Linear classifier\n",
    "In this module, we will start out with arguably the simplest possible function, a linear mapping:\n",
    "\n",
    "$$\n",
    "f(x_i, W, b) = W x_i + b\n",
    "$$\n",
    "\n",
    "### Bias trick\n",
    "\n",
    "Before moving on we want to mention a common simplifying trick to representing the two parameters $ W $ and $ b $ as one. Recall that we defined the score function as:\n",
    "\n",
    "$$\n",
    "f(x_i, W, b) = W x_i + b\n",
    "$$\n",
    "\n",
    "As we proceed through the material it is a little cumbersome to keep track of two sets of parameters (the biases $ b $ and weights $ W $) separately. A commonly used trick is to combine the two sets of parameters into a single matrix that holds both of them by extending the vector $ x_i $ with one additional dimension that always holds the constant 1 - a default bias dimension. With the extra dimension, the new score function will simplify to a single matrix multiply:\n",
    "\n",
    "$$\n",
    "f(x_i, W) = W x_i\n",
    "$$\n",
    "\n",
    "With our CIFAR-10 example, $ x_i $ is now [3073 x 1] instead of [3072 x 1] - (with the extra dimension holding the constant 1), and $ W $ is now [10 x 3073] instead of [10 x 3072]. The extra column that $ W $ now corresponds to the bias $ b $. An illustration might help clarify:\n",
    "\n",
    "![Illustration of Bias Trick](<./img/CS231n/Screenshot 2024-06-12 at 9.13.30 PM.jpg>)\n",
    "\n",
    "Illustration of the bias trick. Doing a matrix multiplication and then adding a bias vector (left) is equivalent to adding a bias dimension with a constant of 1 to all input vectors and extending the weight matrix by 1 column - a bias column (right). Thus, if we preprocess our data by appending ones to all vectors we only have to learn a single matrix of weights instead of two matrices that hold the weights and the biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function \n",
    "\n",
    "A loss function tells how good our current classifier is. Given a dataset of example\n",
    "$$\\begin{align*} \n",
    "\\{( x_{i}, y_{i} )\\}^{N}_{i} = 1\n",
    "\\end{align*}$$\n",
    "\n",
    "where $x_{i}$ is the image and \n",
    "$y_{i}$ is the integer label.\n",
    "\n",
    "Loss over the dataset is a sum of loss over examples: \n",
    "$$\\begin{align*} \\\\\n",
    "L = \\dfrac{1}{N} \\sum_{i} L_{i}\\left(f(x_{i}, W), y_{i} \\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "For example, going back to the example image of a cat and its scores for the classes “cat”, “dog” and “ship”, we saw that the particular set of weights in that example was not very good at all: We fed in the pixels that depict a cat but the cat score came out very low (-96.8) compared to the other classes (dog score 437.9 and ship score 61.95). We are going to measure our unhappiness with outcomes such as this one with a loss function (or sometimes also referred to as the cost function or the objective). Intuitively, the loss will be high if we’re doing a poor job of classifying the training data, and it will be low if we’re doing well.\n",
    "\n",
    "### MultiClass Support Vector Machine Loss\n",
    "The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\\Delta$. Notice that it’s sometimes helpful to anthropomorphise the loss functions as we did above: The SVM “wants” a certain outcome in the sense that the outcome would yield a lower loss (which is good).\n",
    "\n",
    "The multiclass Support Vector Machine (SVM) loss is a method used to evaluate the performance of a model by comparing the scores of different categories. For each example $i$, the loss $L_i$ is calculated by summing over all categories $j$ that are not the true category $Y_i$. The score $S_j$ of each incorrect category is compared with the score $S_{Y_i}$ of the correct category, introducing a safety margin often set to 1. If $S_{Y_i}$ exceeds $S_j$ by at least the margin, the loss is zero. Otherwise, a positive loss is incurred. The total loss for an example is the sum of the losses for all incorrect categories, representing how much the scores of the incorrect categories exceed the margin relative to the correct category. The overall model loss is the average loss over all examples in the training dataset. This loss function can be compactly written as $$L_i = \\sum_{j \\neq Y_i} \\max(0, S_j - S_{Y_i} + 1)$$****, indicating that if $S_j$ is less than $S_{Y_i} - 1$, it contributes zero to the loss; otherwise, it contributes a positive value. The term \"hinge loss\" refers to the shape of the loss function when plotted, showing that when the true category's score is sufficiently high, the loss is zero, and otherwise, it increases linearly. This formulation encourages the model to make confident predictions for the correct classes and penalizes incorrect predictions based on their proximity to the correct score.\n",
    "\n",
    "\n",
    "For example, the score for the $ j $-th class is the $ j $-th element: $ s_j = f(x_i, \\mathbf{W})_j $. The Multiclass SVM loss for the $ i $-th example is then formalized as follows:\n",
    "\n",
    "$$\n",
    "L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)\n",
    "$$\n",
    "\n",
    "**Example:** \n",
    "\n",
    "Let's unpack this with an example to see how it works. Suppose that we have three classes that receive the scores $ s = [13, -7, 11] $, and that the first class is the true class (i.e. $ y_i = 0 $). Also assume that $ \\Delta $ (a hyperparameter we will go into more detail about soon) is 10. The expression above sums over all incorrect classes $ (j \\neq y_i) $, so we get two terms:\n",
    "\n",
    "$$\n",
    "L_i = \\max(0, -7 - 13 + 10) + \\max(0, 11 - 13 + 10)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier \n",
    "\n",
    "The Softmax classifier extends the binary logistic regression to multiple classes. Given the scores \\( s = W x \\), the Softmax function computes probabilities for each class \\( j \\):\n",
    "\n",
    "$$\n",
    "P(y_i = j | x_i; W) = \\frac{e^{s_j}}{\\sum_k e^{s_k}}\n",
    "$$\n",
    "\n",
    "The loss function for the Softmax classifier is the cross-entropy loss:\n",
    "\n",
    "$$\n",
    "L_i = -\\log \\left( \\frac{e^{s_{y_i}}}{\\sum_k e^{s_k}} \\right)\n",
    "$$\n",
    "\n",
    "For full details, including examples and further explanations, visit the provided link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
